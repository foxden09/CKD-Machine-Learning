# -*- coding: utf-8 -*-
"""CKD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FTl0MHcKL-K6lhtuG9sQde-12xG2EEk_

# Preprocessing data
"""

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import numpy as np

data = pd.read_csv('drive/MyDrive/CKD_files/kidney_disease.csv')
data.head(5)

data.info()

data.drop('id', inplace = True, axis=1) #remove id feature
 #change to mean
data.head()

data.var()

#!special encode target column

#replace special chars and convert applicable columns to numeric
data = data.replace('\W', '', regex=True)
data = data.apply(pd.to_numeric, errors='ignore')

#find categorical & numerical columns
cat_cols = data.columns[data.dtypes==object].tolist()
num_cols = data.columns[data.dtypes==np.number].tolist()

#replace missing data | numeric: mean, categorical: mode
data = data.fillna(data.mean(numeric_only=True).round(1))
data[cat_cols] = data[cat_cols].apply(lambda col: col.fillna(col.value_counts().index[0]))

#Gaussian Fit
data[num_cols] = data[num_cols].apply(lambda col: (col-col.mean())/col.std()) 

#encode categorical values 
from sklearn.preprocessing import LabelEncoder
data[cat_cols] = data[cat_cols].apply(LabelEncoder().fit_transform)

#convert object cols to numeric (float or int)
data[cat_cols] = data[cat_cols].apply(pd.to_numeric, errors='raise')
data.info()

# troubleshoot odd characcters
for i in range(0, len(data)-1):
    row = data.iloc[i]
    for x in row: 
      try:
        x = float(x)
      except ValueError as e:
        print(e)
        print(row)
        print(repr(row[x]))

#Split data into X and y
Xy = data.to_numpy()
X, y = np.split(Xy,[-1],axis=1)
y = y.ravel() #fix shape issues

#split into training and testing (80:20)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 101, test_size = 0.2, shuffle = True)

"""# Logistic Regression

SKLearn Model
"""

#@title Cross Validation
#SKLearn Model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate

logRegr = LogisticRegression()

#5 fold cross validation
y_pred = cross_val_predict(logRegr, X, y, cv=5)

#Credit: https://www.section.io/engineering-education/how-to-implement-k-fold-cross-validation/ 
def cross_val(model, X_, y_, cv_):
  ''' Performs K fold cross validation
  Return dictionary of metrics 
  Recall: model's ability to correctly predict the positives out of actual positives '''
  scoring_ = ['accuracy', 'precision', 'recall', 'f1']
  results = cross_validate(estimator=model,
                            X= X_,
                            y= y_,
                            cv= cv_,
                            scoring= scoring_,
                            return_train_score=True)
  
  return {"Training Accuracy scores": results['train_accuracy'],
          "Mean Training Accuracy": results['train_accuracy'].mean()*100,
          "Training Precision scores": results['train_precision'],
          "Mean Training Precision": results['train_precision'].mean(),
          "Training Recall scores": results['train_recall'],
          "Mean Training Recall": results['train_recall'].mean(),
          "Training F1 scores": results['train_f1'],
          "Mean Training F1 Score": results['train_f1'].mean(),
          "Validation Accuracy scores": results['test_accuracy'],
          "Mean Validation Accuracy": results['test_accuracy'].mean()*100,
          "Validation Precision scores": results['test_precision'],
          "Mean Validation Precision": results['test_precision'].mean(),
          "Validation Recall scores": results['test_recall'],
          "Mean Validation Recall": results['test_recall'].mean(),
          "Validation F1 scores": results['test_f1'],
          "Mean Validation F1 Score": results['test_f1'].mean()
          }

logRegr_results = cross_val(logRegr, X, y, 5)
print(logRegr_results['Mean Validation Accuracy'])

#@title Plot SKLearn
import matplotlib.pyplot as plt

def plot_result(x_label, y_label, plot_title, train_data, val_data):
        '''Function to plot a grouped bar chart showing the training and validation
          results of the ML model in each fold after applying K-fold cross-validation.
         Parameters
         ----------
         x_label: str, 
            Name of the algorithm used for training e.g 'Decision Tree'
          
         y_label: str, 
            Name of metric being visualized e.g 'Accuracy'
         plot_title: str, 
            This is the title of the plot e.g 'Accuracy Plot'
         
         train_result: list, array
            This is the list containing either training precision, accuracy, or f1 score.
        
         val_result: list, array
            This is the list containing either validation precision, accuracy, or f1 score.
         Returns
         -------
         The function returns a Grouped Barchart showing the training and validation result
         in each fold.
        '''
        
        # Set size of plot
        plt.figure(figsize=(12,6))
        labels = ["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
        X_axis = np.arange(len(labels))
        ax = plt.gca()
        plt.ylim(0.40000, 1)
        plt.bar(X_axis-0.2, train_data, 0.4, color='lightblue', label='Training')
        plt.bar(X_axis+0.2, val_data, 0.4, color='palevioletred', label='Validation')
        plt.title(plot_title, fontsize=30)
        plt.xticks(X_axis, labels)
        plt.xlabel(x_label, fontsize=14)
        plt.ylabel(y_label, fontsize=14)
        plt.legend()
        plt.grid(True)
        plt.show()

model_name = "Logistic Regression"
plot_result(model_name,
            "Accuracy",
            "Accuracy scores in 5 Folds",
            logRegr_results["Training Accuracy scores"],
            logRegr_results["Validation Accuracy scores"])

#Metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Confusion Matrix
# TP FP
# FN TN
cm = confusion_matrix(y, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

"""Regularized Ng Model"""

#@title Ng Functions
#Credit to Andrew Ng & Coursera for function layout
import math

def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """
    g = 1/(1+np.e**(-z))

    return g

def compute_cost(X, y, w, b):
    """
    Computes the cost over all examples
    Args:
      X : (ndarray Shape (m,n)) data, m examples by n features
      y : (array_like Shape (m,)) target value 
      w : (array_like Shape (n,)) Values of parameters of the model      
      b : scalar Values of bias parameter of the model
    Returns:
      total_cost: (scalar)         sum of all examples' costs 
    """

    m, n = X.shape
    
    z_wb = np.dot(X, w)+b
    f_wb  = sigmoid(z_wb)
    loss = y*(-np.log(f_wb))+(1-y)*(-np.log(1-f_wb))
    total_cost = np.sum(loss)/m

    return total_cost

def compute_cost_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
      X : (array_like Shape (m,n)) data, m examples by n features
      y : (array_like Shape (m,)) target value 
      w : (array_like Shape (n,)) Values of parameters of the model      
      b : (array_like Shape (n,)) Values of bias parameter of the model
      lambda_ : (scalar, float)    Controls amount of regularization
    Returns:
      total_cost: (scalar)         cost 
    """

    m, n = X.shape
    
    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b) 
    
    # You need to calculate this value
    reg_cost = np.sum(np.square(w))
    
    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

    return total_cost

def compute_gradient(X, y, w, b, lambda_=None): 
    """
    Computes the gradient for logistic regression 
 
    Args:
      X : (ndarray Shape (m,n)) variable such as house size 
      y : (array_like Shape (m,1)) actual value 
      w : (array_like Shape (n,1)) values of parameters of the model      
      b : (scalar)                 value of parameter of the model 
    Returns
      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. 
      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. 
    """
    m, n = X.shape
    z_wb = np.dot(X, w)+b
    f_wb = sigmoid(z_wb)
    
    dj_dw = np.dot(f_wb-y, X)/m
    dj_db = np.sum(f_wb-y)/m
    return dj_db, dj_dw

def compute_gradient_reg(X, y, w, b, lambda_ = 1): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X : (ndarray Shape (m,n))   variable such as house size 
      y : (ndarray Shape (m,))    actual value 
      w : (ndarray Shape (n,))    values of parameters of the model      
      b : (scalar)                value of parameter of the model  
      lambda_ : (scalar,float)    regularization constant
    Returns
      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b. 
      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. 

    """
    m, n = X.shape
    
    dj_db, dj_dw = compute_gradient(X, y, w, b)
  
    dj_dw += (lambda_/m)*w
         
    return dj_db, dj_dw

def gradient_descent(X, y, w, b, cost_function, gradient_function, alpha, num_iters): 
    """
    Performs batch gradient descent to learn theta. Updates theta by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      X :    (array_like Shape (m, n)
      y :    (array_like Shape (m,))
      w : (array_like Shape (n,))  Initial values of parameters of the model
      b : (scalar)                 Initial value of parameter of the model
      cost_function:                  function to compute cost
      alpha : (float)                 Learning rate
      num_iters : (int)               number of iterations to run gradient descent
      lambda_ (scalar, float)         regularization constant
      
    Returns:
      w : (array_like Shape (n,)) Updated values of parameters of the model after
          running gradient descent
      b : (scalar)                Updated value of parameter of the model after
          running gradient descent
    """
    
    # number of training examples
    m = len(X)
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w_history = []
   
    for i in range(num_iters):
        # Calculate the gradient and update the parameters
        dj_db, dj_dw = gradient_function(X, y, w, b)   
        
        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               
        b = b - alpha * dj_db              

        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            cost =  cost_function(X, y, w, b)
            J_history.append(cost)

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):
            w_history.append(w)
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.4f}   ")
        
    return w, b, J_history, w_history #return w and J,w history for graphing
  
def predict(X, w, b): 
      """
      Predict whether the label is 0 or 1 using learned logistic
      regression parameters w
      
      Args:
      X : (ndarray Shape (m, n))
      w : (array_like Shape (n,))      Parameters of the model
      b : (scalar, float)              Parameter of the model

      Returns:
      p: (ndarray (m,1))
          The predictions for X using a threshold at 0.5
      """
      # number of training examples
      m, n = X.shape   
      p = np.zeros(m)
    
      # Loop over each example
      z_wb = np.dot(X, w)+b
      f_wb = sigmoid(z_wb)
      p = f_wb >= 0.5
          
      return p

def perform_predict():
  m, n = X_train.shape

  w = np.zeros(n)
  b = 0
  iterations = 12500
  alpha = 0.2

  #regularization to minimize overfitting, permitting higher alpha
  w, b, J_history, w_history = gradient_descent(X_train, y_train, w, b, compute_cost_reg, compute_gradient_reg, alpha, iterations)

  #Print prediction accuracy
  p = predict(X_test, w,b)
  print('\nTest Accuracy: %f'%(np.mean(p == y_test) * 100))

perform_predict()

"""# Random Forest"""

#Credit: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74

from sklearn.ensemble import RandomForestRegressor
from pprint import pprint
rf = RandomForestRegressor(random_state = 42)

"""# Random Forest with GridSearchCV

**Important Parameters **

n_estimators = number of trees in the forest

max_features = max number of features considered for splitting a node

max_depth = max number of levels in each decision tree

min_samples_split = min number of data points placed in a node before the node is split

min_samples_leaf = min number of data points allowed in a leaf node

bootstrap = method for sampling data points (with or without replacement)
"""

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
n_estimators = [int(x) for x in np.linspace(start = 2, stop = 10, num = 10)]
# Number of features to consider at every split
#max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(2, 10, num = 9)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
param_grid = {'n_estimators': n_estimators,
              # 'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(param_grid)

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

CV_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, cv= 5)
CV_rf.fit(X, y) #! Do I use X_train or X

print(CV_rf.best_params_)
print(CV_rf.best_score_)

from sklearn import tree
from graphviz import Source
tree.export_graphviz(CV_rf.best_estimator_.estimators_[2], out_file="iris_tree.dot")
Source.from_file("iris_tree.dot")